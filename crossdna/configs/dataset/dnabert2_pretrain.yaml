_name_: dnabert2_pretrain
text_file: null
dataset_name: dnabert2_pretrain
tokenizer_name: null
cache_dir: null
max_length: 1024
add_eos: True
batch_size: 8  # per GPU
batch_size_eval: ${eval:${.batch_size} * 2}
num_workers: 4  # For preprocessing only
shuffle: True
pin_memory: True
__train_len: ${div_up:1_000_000_000, ${.max_length}}
__l_max: ${.max_length}